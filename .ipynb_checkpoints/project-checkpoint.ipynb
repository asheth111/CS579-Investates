{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports you'll need.\n",
    "from collections import Counter\n",
    "import ConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "from TwitterAPI import TwitterAPI\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "# This method is done for you. Make sure to put your credentials in twitter.cfg.\n",
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in range(0, 16): ## iterate through 16 times to get max No. of tweets\n",
    "\n",
    "def fatch_available_tweets_investor(screen_name):\n",
    "    from datetime import datetime\n",
    "    investor_tweet = {}\n",
    "        \n",
    "    b = []    \n",
    "    for i in range(0, 16):\n",
    "        if i ==0:\n",
    "            request = twitter.request('statuses/user_timeline', {'screen_name': screen_name, 'count': '200' })\n",
    "        else:\n",
    "            request = twitter.request('statuses/user_timeline', {'screen_name': screen_name, 'count': '200', 'max_id' : b[-1]})\n",
    "        if request.status_code == 200:\n",
    "            for tweet in request:\n",
    "                created_at = datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                created_at = str( created_at )\n",
    "                investor_tweet[created_at] = tweet['text']\n",
    "                b.append(tweet['id']) ## append tweet id's\n",
    "        elif request.status_code == 88:\n",
    "            continue\n",
    "        else:\n",
    "            print screen_name\n",
    "            print >> sys.stderr, 'Got error:\\n', request.text, '\\nsleeping for 15 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)\n",
    "        \n",
    "    return investor_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(list_of_dictionary):\n",
    "    '''\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    '''\n",
    "    result = {}\n",
    "    for dictionary in list_of_dictionary:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error:\n",
      "{\"errors\":[{\"message\":\"Rate limit exceeded\",\"code\":88}]} \n",
      "sleeping for 15 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gregorymckenna\n",
      "\n",
      "Benzinga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error:\n",
      "{\"errors\":[{\"message\":\"Rate limit exceeded\",\"code\":88}]} \n",
      "sleeping for 15 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TheStreet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error:\n",
      "{\"errors\":[{\"message\":\"Rate limit exceeded\",\"code\":88}]} \n",
      "sleeping for 15 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vkhosla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error:\n",
      "{\"errors\":[{\"message\":\"Rate limit exceeded\",\"code\":88}]} \n",
      "sleeping for 15 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Do not run this block, this is fatching all tweeter data, this will take more then an hour\n",
    "#       We have already downloaded data, go down and run the block as suggested to load all data in next to next block \n",
    "\n",
    "\n",
    "#faatch tweets from users given in \"input.txt\"\n",
    "#and generate \"data.txt\" file with all tweets \n",
    "#in jason formate like 'created date-time' -> 'tweet text'\n",
    "#also returns a complete \n",
    "\n",
    "result = []\n",
    "fopen = open(\"input.txt\")\n",
    "fopen.seek(0)\n",
    "for line in fopen:\n",
    "    tuples = fatch_available_tweets_investor(line)\n",
    "    result.append(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge all tweets from all investors in one single object\n",
    "all_tweets = merge_dicts(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sorts all tweet in chronological order\n",
    "#Again\n",
    "# NOTE: Do not run this block, We have already done this part and stored data pickle, run next block instead \n",
    "\n",
    "print len(all_tweets)\n",
    "sorted_all_tweets = sorted(all_tweets.items(), key=operator.itemgetter(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just run this code for loading all tweets\n",
    "import pickle\n",
    "# We dumped all data by this command\n",
    "#pickle.dump(sorted_all_tweets, open( \"sorted_all_tweets.p\", \"wb\" ))\n",
    "\n",
    "sorted_all_tweets = pickle.load(open('sorted_all_tweets.p', 'rb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2009-02-14 22:52:30', u'Researching a company for potential investment.')\n"
     ]
    }
   ],
   "source": [
    "#sample of sorted tweets/testing data\n",
    "print sorted_all_tweets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store all data in json formate as well, in data.txt\n",
    "import json\n",
    "outf = open('data.txt', 'wt')\n",
    "json.dump(sorted_all_tweets , outf, indent=4)\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(text):\n",
    "    \"\"\"Given a string, return a list of tokens such that: (1) all\n",
    "    tokens are lowercase, (2) all punctuation, special characters and numbers are removed. Only alphabates are considered\n",
    "    Params:\n",
    "        text....a string\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    ###\n",
    "    text = text.lower()\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', text).split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'tirth', 'gk', 'mayank']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How our tokenize function works\n",
    "tokenize(\"my name is tirth 1234 5:4 @%^*&%#^&gk mayank? !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized 2284 tweets. found 1880 terms.\n"
     ]
    }
   ],
   "source": [
    "# Exploring and doing Proof of concepts by taking 2010 data\n",
    "\n",
    "#tweets_2009 = [(key,value) for key, value in sorted_all_tweets if key > '2009-01-01' and key < '2009-12-12']\n",
    "tweets_2010 = [(key,value) for key, value in sorted_all_tweets if key > '2010-01-01' and key < '2010-12-31']\n",
    "\n",
    "# Create feature vectors (X)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#after few itterations and trial and error, we decided to keep min_df=2, max_df = 7 for better results\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, min_df=2, max_df = 7)\n",
    "X = vectorizer.fit_transform(t[1] for t in tweets_2010)\n",
    "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_indices= [1743  745 1799 ...,  630  629  939]\n",
      "top_terms:\n",
      "unicef 1743\n",
      "google 745\n",
      "wall 1799\n",
      "might 1089\n",
      "being 162\n",
      "pricing 1313\n",
      "process 1319\n",
      "haiti 774\n",
      "without 1834\n",
      "information 878\n",
      "before 156\n",
      "uk 1733\n",
      "trying 1722\n",
      "hk 814\n",
      "mateo 1059\n",
      "ready 1372\n",
      "email 523\n",
      "both 199\n",
      "elite 519\n",
      "nasdaq 1143\n"
     ]
    }
   ],
   "source": [
    "# What are the most frequent terms in our sample tweets_2010?\n",
    "import numpy as np\n",
    "# Sum columns:\n",
    "col_sums = X.sum(axis=0).tolist()[0]\n",
    "# Sort sums in descending order, and return the indices.\n",
    "top_indices = np.argsort(col_sums)[::-1]\n",
    "print 'top_indices=', top_indices\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "top_terms = vocab[top_indices]\n",
    "print 'top_terms:\\n', '\\n'.join('%s %d' % (term, count) for term, count in zip(top_terms, top_indices)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agribusiness': ['agribusiness',\n",
       "  'crop',\n",
       "  'production',\n",
       "  'basic',\n",
       "  'processing',\n",
       "  'vegetables',\n",
       "  'fruits',\n",
       "  'sugar',\n",
       "  'cane',\n",
       "  'sugar',\n",
       "  'beets',\n",
       "  'tobacco',\n",
       "  'dairy',\n",
       "  'poultry',\n",
       "  'eggs',\n",
       "  'livestock',\n",
       "  'agricultural',\n",
       "  'services',\n",
       "  'products',\n",
       "  'farm',\n",
       "  'bureaus',\n",
       "  'food',\n",
       "  'processing',\n",
       "  'sales',\n",
       "  'food',\n",
       "  'products',\n",
       "  'manufacturing',\n",
       "  'food',\n",
       "  'stores',\n",
       "  'meat',\n",
       "  'processing',\n",
       "  'products',\n",
       "  'forestry',\n",
       "  'forest',\n",
       "  'products'],\n",
       " 'communicationsandelectronics': ['communicationsandelectronics',\n",
       "  'printing',\n",
       "  'publishing',\n",
       "  'books',\n",
       "  'magazines',\n",
       "  'newspapers',\n",
       "  'tv',\n",
       "  'movies',\n",
       "  'music',\n",
       "  'cable',\n",
       "  'satellite',\n",
       "  'tv',\n",
       "  'production',\n",
       "  'distribution',\n",
       "  'commercial',\n",
       "  'tv',\n",
       "  'radio',\n",
       "  'stations',\n",
       "  'motion',\n",
       "  'picture',\n",
       "  'production',\n",
       "  'distribution',\n",
       "  'recorded',\n",
       "  'music',\n",
       "  'music',\n",
       "  'production',\n",
       "  'tv',\n",
       "  'production',\n",
       "  'telephone',\n",
       "  'utilities',\n",
       "  'telecom',\n",
       "  'services',\n",
       "  'equipment',\n",
       "  'electronics',\n",
       "  'manufacturing',\n",
       "  'equipment',\n",
       "  'internet'],\n",
       " 'construction': ['construction',\n",
       "  'contractors',\n",
       "  'home',\n",
       "  'builders',\n",
       "  'contractors',\n",
       "  'construction',\n",
       "  'architectural',\n",
       "  'building',\n",
       "  'materials',\n",
       "  'equipment'],\n",
       " 'defence': ['defence',\n",
       "  'defence',\n",
       "  'aerospace',\n",
       "  'war',\n",
       "  'terrorist',\n",
       "  'plane',\n",
       "  'helicopter',\n",
       "  'defense',\n",
       "  'gun'],\n",
       " 'education': ['education',\n",
       "  'non',\n",
       "  'profit',\n",
       "  'education',\n",
       "  'teacher',\n",
       "  'student',\n",
       "  'college',\n",
       "  'university',\n",
       "  'exam',\n",
       "  'book',\n",
       "  'books',\n",
       "  'study',\n",
       "  'laptop',\n",
       "  'professor',\n",
       "  'math',\n",
       "  'science',\n",
       "  'technology',\n",
       "  'history',\n",
       "  'physics',\n",
       "  'marks',\n",
       "  'grade',\n",
       "  'result'],\n",
       " 'energyandnaturalresources': ['energyandnaturalresources',\n",
       "  'oil',\n",
       "  'gas',\n",
       "  'natural',\n",
       "  'pipelines',\n",
       "  'coal',\n",
       "  'mining',\n",
       "  'energy',\n",
       "  'electric',\n",
       "  'utilities',\n",
       "  'waste',\n",
       "  'management'],\n",
       " 'entertainment': ['entertainment',\n",
       "  'recreation',\n",
       "  'sports',\n",
       "  'arenas',\n",
       "  'equipment',\n",
       "  'casinos',\n",
       "  'gambling',\n",
       "  'gaming',\n",
       "  'movie',\n",
       "  'game',\n",
       "  'fun',\n",
       "  'television',\n",
       "  'tv'],\n",
       " 'finance': ['finance',\n",
       "  'insurance',\n",
       "  'real',\n",
       "  'estate',\n",
       "  'commercial',\n",
       "  'banks',\n",
       "  'savings',\n",
       "  'loans',\n",
       "  'credit',\n",
       "  'unions',\n",
       "  'finance',\n",
       "  'credit',\n",
       "  'companies',\n",
       "  'loan',\n",
       "  'companies',\n",
       "  'payday',\n",
       "  'lenders',\n",
       "  'investment',\n",
       "  'venture',\n",
       "  'capital',\n",
       "  'hedge',\n",
       "  'funds',\n",
       "  'equity',\n",
       "  'firms',\n",
       "  'insurance',\n",
       "  'real',\n",
       "  'estate',\n",
       "  'mortgage',\n",
       "  'bankers',\n",
       "  'brokers',\n",
       "  'accountants',\n",
       "  'business'],\n",
       " 'food': ['food',\n",
       "  'beverage',\n",
       "  'restaurants',\n",
       "  'drinking',\n",
       "  'beer',\n",
       "  'wine',\n",
       "  'liquor',\n",
       "  'beef',\n",
       "  'bacon',\n",
       "  'turkey',\n",
       "  'chips',\n",
       "  'sandwich',\n",
       "  'pop',\n",
       "  'pepsi',\n",
       "  'donut',\n",
       "  'coffee',\n",
       "  'beans',\n",
       "  'tea',\n",
       "  'vegetables',\n",
       "  'meat',\n",
       "  'vegan',\n",
       "  'taste',\n",
       "  'spicies',\n",
       "  'spicy',\n",
       "  'seafood'],\n",
       " 'health': ['health',\n",
       "  'health',\n",
       "  'insurance',\n",
       "  'professionals',\n",
       "  'chiropractors',\n",
       "  'dentists',\n",
       "  'nurses',\n",
       "  'hospitals',\n",
       "  'nursing',\n",
       "  'homes',\n",
       "  'hmos',\n",
       "  'pharmaceuticals',\n",
       "  'medical',\n",
       "  'supplies',\n",
       "  'nutritional',\n",
       "  'dietary',\n",
       "  'supplements',\n",
       "  'manufacturing'],\n",
       " 'ideologicalandsingleissue': ['ideologicalandsingleissue',\n",
       "  'republican',\n",
       "  'conservative',\n",
       "  'democratic',\n",
       "  'liberal',\n",
       "  'leadership',\n",
       "  'pacs',\n",
       "  'democratic',\n",
       "  'republican',\n",
       "  'leadership',\n",
       "  'pacs',\n",
       "  'foreign',\n",
       "  'defense',\n",
       "  'policy',\n",
       "  'pro',\n",
       "  'israel',\n",
       "  'women',\n",
       "  's',\n",
       "  'human',\n",
       "  'gay',\n",
       "  'lesbian',\n",
       "  'environment',\n",
       "  'control',\n",
       "  'rights',\n",
       "  'issue',\n",
       "  'abortion',\n",
       "  'republican',\n",
       "  'democratic'],\n",
       " 'informationtechnology': ['informationtechnology',\n",
       "  'ip',\n",
       "  'programming',\n",
       "  'security',\n",
       "  'information',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'algorithm',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'parallel',\n",
       "  'computing',\n",
       "  'multimedia',\n",
       "  'systems',\n",
       "  'virtual',\n",
       "  'reality',\n",
       "  'software',\n",
       "  'open',\n",
       "  'source',\n",
       "  'graphics',\n",
       "  'hacking',\n",
       "  'emulators',\n",
       "  'computer',\n",
       "  'software',\n",
       "  'database',\n",
       "  'programming',\n",
       "  'code',\n",
       "  'coding',\n",
       "  'front',\n",
       "  'end',\n",
       "  'back',\n",
       "  'end'],\n",
       " 'labor': ['labor',\n",
       "  'building',\n",
       "  'trade',\n",
       "  'unions',\n",
       "  'industrial',\n",
       "  'transportation',\n",
       "  'air',\n",
       "  'transport',\n",
       "  'public',\n",
       "  'sector',\n",
       "  'postal',\n",
       "  'teachers',\n",
       "  'lawyers',\n",
       "  'lobbyists',\n",
       "  'lawyers',\n",
       "  'law',\n",
       "  'firms',\n",
       "  'judge'],\n",
       " 'manufacturing': ['manufacturing',\n",
       "  'steel',\n",
       "  'production',\n",
       "  'manufacturing',\n",
       "  'distributing',\n",
       "  'clothing',\n",
       "  'textiles',\n",
       "  'machine',\n",
       "  'equipment',\n",
       "  'product'],\n",
       " 'tourism': ['tourism',\n",
       "  'lodging',\n",
       "  'tourist',\n",
       "  'tour',\n",
       "  'beach',\n",
       "  'hotel',\n",
       "  'motel',\n",
       "  'room',\n",
       "  'advertising',\n",
       "  'rental',\n",
       "  'rent',\n",
       "  'food',\n",
       "  'massage',\n",
       "  'spa'],\n",
       " 'transportation': ['transportation',\n",
       "  'transport',\n",
       "  'airlines',\n",
       "  'automotive',\n",
       "  'auto',\n",
       "  'manufacturers',\n",
       "  'car',\n",
       "  'dealers',\n",
       "  'trucking',\n",
       "  'rail',\n",
       "  'roads',\n",
       "  'sea',\n",
       "  'cruise',\n",
       "  'ships',\n",
       "  'lines',\n",
       "  'goods',\n",
       "  'cars',\n",
       "  'pool',\n",
       "  'passenger',\n",
       "  'traffic',\n",
       "  'highways',\n",
       "  'trailer',\n",
       "  'bus',\n",
       "  'truck']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Training dataset from our industryList.txt file\n",
    "\n",
    "industryDictionary = {}\n",
    "fopen = open('industryList.txt')\n",
    "fopen.seek(0)\n",
    "for line in fopen:\n",
    "    industryDetails = tokenize(line)\n",
    "    industryDictionary[industryDetails[0]] = industryDetails\n",
    "\n",
    "industryDictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2284x1880 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 6045 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agribusiness': 11,\n",
       " 'communicationsandelectronics': 25,\n",
       " 'construction': 5,\n",
       " 'defence': 3,\n",
       " 'education': 33,\n",
       " 'energyandnaturalresources': 10,\n",
       " 'entertainment': 6,\n",
       " 'finance': 23,\n",
       " 'food': 3,\n",
       " 'ideologicalandsingleissue': 34,\n",
       " 'informationtechnology': 30,\n",
       " 'labor': 26,\n",
       " 'manufacturing': 9,\n",
       " 'tourism': 11,\n",
       " 'transportation': 8}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proof of concepts of our Analizer by checking terms into our training dataset \n",
    "#and deciding which terms are frequent in tweets for our tweet_2010 \n",
    "\n",
    "resultDirectory = {} \n",
    "for i in range(X.get_shape()[0]):\n",
    "        for j in range(X.get_shape()[1]):\n",
    "            if X[i,j]>0:\n",
    "                for k,v in industryDictionary.iteritems():\n",
    "                        if  vocab[j] in v:\n",
    "                            if k in resultDirectory:  \n",
    "                                resultDirectory[k] = resultDirectory[k] + X[i,j]\n",
    "                            else:\n",
    "                                resultDirectory[k] = X[i,j]\n",
    "resultDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create feature vectors (X) for tweets\n",
    "'''\n",
    "input: List of tweets\n",
    "output: a sparce matrix and vocab of that matrix\n",
    "'''\n",
    "\n",
    "def createSparseMatrix(tweets):    \n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize, min_df=2, max_df = 7)\n",
    "    X = vectorizer.fit_transform(t[1] for t in tweets)\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    return X, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to analyze input tweeter set and deciding poopular sectors of industry\n",
    "'''\n",
    "input:  X      - sparce mateix of tweets and terms\n",
    "        vocab  - vocabulary of the sparce matrix\n",
    "        industryDictionary - training dataset as directory of industry(key): related terms(values)\n",
    "        \n",
    "output: resultDirectory - output as directory of list of industry(key): popularity rank(value) #just considered occarance for now\n",
    "'''\n",
    "\n",
    "def calculatePopularSectors(X, vocab, industryDictionary):\n",
    "    resultDirectory = {} \n",
    "    for i in range(X.get_shape()[0]):\n",
    "            for j in range(X.get_shape()[1]):\n",
    "                if X[i,j]>0:\n",
    "                    for k,v in industryDictionary.iteritems():\n",
    "                            if  vocab[j] in v:\n",
    "                                if k in resultDirectory:  \n",
    "                                    resultDirectory[k] = resultDirectory[k] + X[i,j]\n",
    "                                else:\n",
    "                                    resultDirectory[k] = X[i,j]\n",
    "    return resultDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NOTE: Do not run this block as it contains analysis of almost 100,000 tweets by comparing terms of each, \n",
    "#      It takes too much time. We have run this part and included in our presentation and report.\n",
    "#      So, instead run the next block which takes random 3000 samples of each qarter and give us results     \n",
    "#Final run for all quater wise analysis\n",
    "\n",
    "def finalQuarterlyAnalysis(sorted_all_tweets,industryDictionary,startyear = 2015):\n",
    "    resultDirectory = {}\n",
    "    #for year in range(startyear,2016):\n",
    "    tweets_Q1 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-01-01' and key < str(startyear)+'-03-31']\n",
    "    X1, vocab1 = createSparseMatrix(tweets_Q1)\n",
    "    print 'Q1 tweets vectorized %d tweets. found %d terms.' % (X1.shape[0], X1.shape[1])\n",
    "    Q1result = calculatePopularSectors(X1, vocab1, industryDictionary)\n",
    "    resultDirectory['1stQuarter 2015'] = Q1result\n",
    "\n",
    "    tweets_Q2 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-04-01' and key < str(startyear)+'-06-30']\n",
    "    X2, vocab2 = createSparseMatrix(tweets_Q2)\n",
    "    print 'Q2 tweets vectorized %d tweets. found %d terms.' % (X2.shape[0], X2.shape[1])\n",
    "    Q2result = calculatePopularSectors(X2, vocab2, industryDictionary)\n",
    "    resultDirectory['2ndQuarter 2015'] = Q2result\n",
    "\n",
    "    tweets_Q3 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-07-01' and key < str(startyear)+'-09-30']\n",
    "    X3, vocab3 = createSparseMatrix(tweets_Q3)\n",
    "    print 'Q3 tweets vectorized %d tweets. found %d terms.' % (X3.shape[0], X3.shape[1])\n",
    "    Q3result = calculatePopularSectors(X3, vocab3, industryDictionary)\n",
    "    resultDirectory['3rdQuarter 2015'] = Q3result\n",
    "\n",
    "    tweets_Q4 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-10-01' and key < str(startyear)+'-12-31']\n",
    "    X4, vocab4 = createSparseMatrix(tweets_Q4)\n",
    "    print 'Q1 tweets vectorized %d tweets. found %d terms.' % (X4.shape[0], X4.shape[1])\n",
    "    Q4result = calculatePopularSectors(X4, vocab4, industryDictionary)\n",
    "    resultDirectory['4thQuarter 2015'] = Q4result\n",
    "    \n",
    "    return resultDirectory\n",
    "        \n",
    "resultDirectory = finalQuarterlyAnalysis(sorted_all_tweets,industryDictionary,startyear = 2015)\n",
    "\n",
    "resultDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 tweets vectorized 500 tweets. found 646 terms.\n",
      "Q2 tweets vectorized 500 tweets. found 643 terms.\n",
      "Q3 tweets vectorized 500 tweets. found 657 terms.\n",
      "Q1 tweets vectorized 500 tweets. found 670 terms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1stQuarter 2015': {'agribusiness': 2,\n",
       "  'communicationsandelectronics': 4,\n",
       "  'education': 14,\n",
       "  'energyandnaturalresources': 9,\n",
       "  'entertainment': 2,\n",
       "  'finance': 33,\n",
       "  'food': 2,\n",
       "  'health': 5,\n",
       "  'ideologicalandsingleissue': 7,\n",
       "  'informationtechnology': 22,\n",
       "  'labor': 13,\n",
       "  'manufacturing': 2,\n",
       "  'transportation': 2},\n",
       " '2ndQuarter 2015': {'agribusiness': 18,\n",
       "  'communicationsandelectronics': 11,\n",
       "  'construction': 2,\n",
       "  'education': 21,\n",
       "  'entertainment': 6,\n",
       "  'finance': 27,\n",
       "  'food': 3,\n",
       "  'health': 3,\n",
       "  'ideologicalandsingleissue': 12,\n",
       "  'informationtechnology': 23,\n",
       "  'labor': 5,\n",
       "  'manufacturing': 10,\n",
       "  'tourism': 3,\n",
       "  'transportation': 4},\n",
       " '3rdQuarter 2015': {'agribusiness': 3,\n",
       "  'communicationsandelectronics': 5,\n",
       "  'education': 9,\n",
       "  'energyandnaturalresources': 6,\n",
       "  'entertainment': 3,\n",
       "  'finance': 22,\n",
       "  'food': 2,\n",
       "  'ideologicalandsingleissue': 6,\n",
       "  'informationtechnology': 20,\n",
       "  'labor': 9},\n",
       " '4thQuarter 2015': {'agribusiness': 10,\n",
       "  'communicationsandelectronics': 9,\n",
       "  'construction': 5,\n",
       "  'education': 5,\n",
       "  'energyandnaturalresources': 8,\n",
       "  'entertainment': 11,\n",
       "  'finance': 18,\n",
       "  'food': 4,\n",
       "  'health': 2,\n",
       "  'ideologicalandsingleissue': 11,\n",
       "  'informationtechnology': 4,\n",
       "  'labor': 17,\n",
       "  'manufacturing': 2,\n",
       "  'tourism': 5,\n",
       "  'transportation': 2}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking random samples from each quarter and calculating with our analysier to get most popular sectors in investors\n",
    "\n",
    "import random\n",
    "\n",
    "def finalQuarterlyAnalysis(sorted_all_tweets,industryDictionary,startyear = 2015):\n",
    "    resultDirectory = {}\n",
    "    #for year in range(startyear,2016):\n",
    "    tweets_Q1 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-01-01' and key < str(startyear)+'-03-31']\n",
    "    random.shuffle(tweets_Q1)\n",
    "    tweets_Q1 = tweets_Q1[0:3000]\n",
    "    #print tweets_Q1\n",
    "    X1, vocab1 = createSparseMatrix(tweets_Q1)\n",
    "    print 'Q1 tweets vectorized %d tweets. found %d terms.' % (X1.shape[0], X1.shape[1])\n",
    "    Q1result = calculatePopularSectors(X1, vocab1, industryDictionary)\n",
    "    resultDirectory['1stQuarter 2015'] = Q1result\n",
    "    \n",
    "    tweets_Q2 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-04-01' and key < str(startyear)+'-06-30']\n",
    "    random.shuffle(tweets_Q2)\n",
    "    tweets_Q2 = tweets_Q2[0:3000]\n",
    "    X2, vocab2 = createSparseMatrix(tweets_Q2)\n",
    "    print 'Q2 tweets vectorized %d tweets. found %d terms.' % (X2.shape[0], X2.shape[1])\n",
    "    Q2result = calculatePopularSectors(X2, vocab2, industryDictionary)\n",
    "    resultDirectory['2ndQuarter 2015'] = Q2result\n",
    "\n",
    "    tweets_Q3 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-07-01' and key < str(startyear)+'-09-30']\n",
    "    random.shuffle(tweets_Q3)\n",
    "    tweets_Q3 = tweets_Q3[0:3000]\n",
    "    X3, vocab3 = createSparseMatrix(tweets_Q3)\n",
    "    print 'Q3 tweets vectorized %d tweets. found %d terms.' % (X3.shape[0], X3.shape[1])\n",
    "    Q3result = calculatePopularSectors(X3, vocab3, industryDictionary)\n",
    "    resultDirectory['3rdQuarter 2015'] = Q3result\n",
    "\n",
    "    tweets_Q4 = [(key,value) for key, value in sorted_all_tweets if key > str(startyear)+'-10-01' and key < str(startyear)+'-12-31']\n",
    "    random.shuffle(tweets_Q4)\n",
    "    tweets_Q4 = tweets_Q4[0:3000]\n",
    "    X4, vocab4 = createSparseMatrix(tweets_Q4)\n",
    "    print 'Q1 tweets vectorized %d tweets. found %d terms.' % (X4.shape[0], X4.shape[1])\n",
    "    Q4result = calculatePopularSectors(X4, vocab4, industryDictionary)\n",
    "    resultDirectory['4thQuarter 2015'] = Q4result\n",
    "\n",
    "    \n",
    "    \n",
    "    return resultDirectory\n",
    "        \n",
    "resultDirectory = finalQuarterlyAnalysis(sorted_all_tweets,industryDictionary,startyear = 2015)\n",
    "\n",
    "resultDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-40b8aee8999c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultDirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindustry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.pyc\u001b[0m in \u001b[0;36mbar\u001b[1;34m(left, height, width, bottom, hold, **kwargs)\u001b[0m\n\u001b[0;32m   2571\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2572\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2573\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2574\u001b[0m         \u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2575\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.pyc\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, left, height, width, bottom, **kwargs)\u001b[0m\n\u001b[0;32m   2043\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpolation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m             \u001b[1;31m#print r.get_label(), label, 'label' in kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2045\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2046\u001b[0m             \u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.pyc\u001b[0m in \u001b[0;36madd_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1569\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1570\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.pyc\u001b[0m in \u001b[0;36m_update_patch_limits\u001b[1;34m(self, patch)\u001b[0m\n\u001b[0;32m   1586\u001b[0m         \u001b[0mvertices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m             \u001b[0mxys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m                 patch_to_data = (patch.get_data_transform() -\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\patches.pyc\u001b[0m in \u001b[0;36mget_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\patches.pyc\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mrot_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_angle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tirth Patel\\Anaconda\\lib\\site-packages\\matplotlib\\transforms.pyc\u001b[0m in \u001b[0;36mfrom_bounds\u001b[1;34m(x0, y0, width, height)\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \"\"\"\n\u001b[1;32m--> 829\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1331ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "industry = []\n",
    "trend = []\n",
    "for items in resultDirectory:\n",
    "    industry.append(items)\n",
    "    trend.append(resultDirectory[items])\n",
    "\n",
    "x = np.arange(len(resultDirectory))\n",
    "\n",
    "plt.bar(x, trend)\n",
    "chart = plt.xticks(x + 0.5, [item.capitalize() for item in industry],  rotation= 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
